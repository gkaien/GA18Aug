{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is to identify the attributes/characteristics that have high tendency of carring out fraudulent credit card transactions based on the dataset collected.\n",
    "\n",
    "The hypothesis is that some of the demographic factors play an important clues to classify card users into differnt risk level of credit card default groupings.\n",
    "\n",
    "Dataset Information:\n",
    "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005\n",
    "The target column is the 'default.payment.next.month'\n",
    "\n",
    "There are 25 variables:\n",
    "ID: ID of each client\n",
    "LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
    "SEX: Gender (1=male, 2=female)\n",
    "EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "AGE: Age in years\n",
    "PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "default.payment.next.month: Default payment (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "from math import log\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit= pd.read_csv('/Users/kaiengwee/Documents/GitHub/GA18Aug/Project4/UCI_Credit_Card.csv')\n",
    "Credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.rename(columns={'default.payment.next.month':'default_nextMTH'},inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit['default_nextMTH'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_count = Credit['default_nextMTH'].value_counts()\n",
    "print('No default in next month:', target_count[0])\n",
    "print('Default in next month:', target_count[1])\n",
    "print('Proportion of default cases in the dataset:', round(target_count[1] / target_count[0], 2), ': 1')\n",
    "\n",
    "target_count.plot(kind='bar', title='Count (target column)');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(Credit.corr(),  annot=True, vmin=-1, vmax= 1, cmap='PiYG', center= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap above, BILL_AMT1 to BILL_AMT6 are highly correlated within themselves. Since BILL_AMT1 is the \n",
    "latest billing month before default occured, I will drop out BILL_AMT2 to BILL_AMT6. In addition, ID and AGE features will \n",
    "also be removed since it doesn't mean anything in predicting the default occurence and lower correlated to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.drop(['ID','AGE','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20,20))\n",
    "sns.heatmap(Credit.corr(),  annot=True, vmin=-1, vmax= 1, cmap='PiYG', center= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_d = 0\n",
    "count_nd = 0\n",
    "\n",
    "for i in Credit['default_nextMTH']:\n",
    "    if i == 1:\n",
    "        count_d += 1\n",
    "    else:\n",
    "        count_nd += 1\n",
    "    \n",
    "total= count_d + count_nd\n",
    "print(\"Total cases under study\", total)\n",
    "print(\"Total default cases\", count_d,\"or \", round(count_d/total,2)*100,\"%\" )\n",
    "print(\"Total non-default cases\", count_nd,\"or \", round(count_nd/total,2)*100,\"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The remaining features have 18 columns\n",
    "Credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEX: Gender (1=male, 2=female), the data set in SEX column only has either male or female\n",
    "Credit['SEX'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEX_dummies= pd.get_dummies(Credit.SEX, prefix= 'SEX')\n",
    "SEX_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop SEX_2 since one column is enough to represent the two genders\n",
    "\n",
    "SEX_dummies.drop(SEX_dummies.columns[-1], axis=1, inplace= True)\n",
    "SEX_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "# value counts shows there is redundant categories under EDUCATION column i.e. categories 0, and 6\n",
    "Credit['EDUCATION'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to replace the category 0 with NA\n",
    "\n",
    "Credit.EDUCATION.replace(0, np.nan, inplace=True)\n",
    "Credit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with NA\n",
    "\n",
    "Credit.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column EDUCATION after removing the rows with NA\n",
    "\n",
    "Credit['EDUCATION'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create EDUCATION DUMMY COLUMNS\n",
    "\n",
    "EDUCATION_dummies= pd.get_dummies(Credit.EDUCATION, prefix= 'EDUCATION')\n",
    "EDUCATION_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop EDUCATION_0 & EDUCATION_6, columns with unknown\n",
    "# Drop EDUCATION_5 since EDUCATION 1, 2, 3, and 4 columns are enough to represent all categories of EDUCATION\n",
    "\n",
    "EDUCATION_dummies.drop(['EDUCATION_5.0','EDUCATION_6.0'], axis=1, inplace= True)\n",
    "EDUCATION_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "# value counts shows there is redundant categories under MARRIAGE column i.e. categories 0\n",
    "\n",
    "Credit['MARRIAGE'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to replace the category 0 with NA\n",
    "\n",
    "Credit.MARRIAGE.replace(0, np.nan, inplace=True)\n",
    "Credit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with NA\n",
    "\n",
    "Credit.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column MARRIAGE after removing the rows with NA\n",
    "\n",
    "Credit['MARRIAGE'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARRIAGE_dummies= pd.get_dummies(Credit.MARRIAGE, prefix= 'MARRIAGE')\n",
    "MARRIAGE_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop MARRIAGE_3 since MARRIAGE 1 and 2 are enough to represent all categories of EDUCATION\n",
    "\n",
    "MARRIAGE_dummies.drop(MARRIAGE_dummies.columns[-1], axis=1, inplace= True)\n",
    "MARRIAGE_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the dataset Credit are clear from cell with NA\n",
    "\n",
    "Credit.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the latest shape of data set Credit has lower number of rows at 29932\n",
    "\n",
    "Credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the Credit with three sets of dummies created namely SEX, EDUCATION and MARRIAGE\n",
    "\n",
    "Credit_dummies= pd.concat([Credit, SEX_dummies, EDUCATION_dummies, MARRIAGE_dummies], axis=1)\n",
    "Credit_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the origianl columns for SEX, EDUCATION and MARRIAGE\n",
    "\n",
    "Credit_dummies.drop(['SEX','EDUCATION','MARRIAGE'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the target column ('default_nextMTH') to the last column\n",
    "\n",
    "y= Credit_dummies.default_nextMTH\n",
    "Credit_dummies.drop(['default_nextMTH'], axis=1, inplace= True)\n",
    "Credit_dummies= pd.concat([Credit_dummies, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the columns in data set named as Credit_dummies\n",
    "\n",
    "Credit_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of Credit_dummies after added the three dummy columns\n",
    "\n",
    "Credit_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X)\n",
    "feature_cols= Credit_dummies.columns.drop(['default_nextMTH'])\n",
    "X= Credit_dummies[feature_cols]\n",
    "\n",
    "# Create response vector (y)\n",
    "\n",
    "y= Credit_dummies.default_nextMTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((type(X)))\n",
    "print((type(X.values)))\n",
    "print((type(y)))\n",
    "print((type(y.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X.shape))\n",
    "print((y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Credit_dummies contain any NA cell\n",
    "\n",
    "Credit_dummies.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_pred = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data set for Train and Test sets using Logistic Regression and check their accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "model = logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing score: '  + str(logreg.score(X_test, y_test)))\n",
    "print('Training score: '  + str(logreg.score(X_train, y_train)))\n",
    "list(zip(feature_cols, logreg.feature_importances_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Training and Testing scores are almost equal to the mix of none default cases in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg.intercept_)\n",
    "print(logreg.coef_)\n",
    "coeff = pd.DataFrame(dict(zip(X.columns,model.coef_[0])),index=[0])\n",
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "cm=metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"Actual: No\", \"Actual: Yes\"]\n",
    "list2 = [\"Predicted: No\", \"Predicted: Yes\"]\n",
    "pd.DataFrame(cm, list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report simply shows the model can only predict on actual data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('default_nextMTH Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using another sklearn linear model to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model, model_selection, metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.25, random_state=46)\n",
    "logit_simple = linear_model.LogisticRegression(C=1e9).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1- y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our accuracy on the test set?\n",
    "print(np.mean(y_test == logit_simple.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions.\n",
    "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_true=y_test, y_pred=logit_pred_proba > .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sklearn linear model is built on logistic regression and hence, is showing the same accuracy at 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply RandomForest to get model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train model\n",
    "Random = RandomForestClassifier(n_estimators=10, max_depth= 10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "model= Random.fit(X_train,y_train)\n",
    " \n",
    "# Predict on training set\n",
    "y_pred = Random.predict(X_test)\n",
    " \n",
    "print('Testing score: '  + str(Random.score(X_test, y_test)))\n",
    "print('Training score: '  + str(Random.score(X_train, y_train)))\n",
    "list(zip(feature_cols, Random.feature_importances_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance check does not show any particular feature having significant importance, in other words, the features \n",
    "are having similar weights on the target column. \n",
    "The most importance featur is PAY_0 which makes sense as this is the latest bill payment. The least important feature\n",
    "is EDUCATION category 4, because of data availability issue.\n",
    "Random forest can work better with categorical features, than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the test data set accuracy using roc auc curve after modeled by Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "cm=metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"Actual: No\", \"Actual: Yes\"]\n",
    "list2 = [\"Predicted: No\", \"Predicted: Yes\"]\n",
    "pd.DataFrame(cm, list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision is good but the recall is very low for Class 1 (default class) because the data set is imbalanced\n",
    "In other words, the model is only good in predicting none default case only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('default_nextMTH Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Smote Tomek on imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "X= Credit_dummies[feature_cols]\n",
    "y= Credit_dummies.default_nextMTH\n",
    "\n",
    "# Generate the dataset\n",
    "#X, y = make_classification(n_classes=2, class_sep=2, weights=[0.22, 0.78],\n",
    "                           #n_informative=3, n_redundant=1, flip_y=0,\n",
    "                           #n_features=18, n_clusters_per_class=1,\n",
    "                           #n_samples=30000, random_state=10)\n",
    "\n",
    "# Instanciate a PCA object for the sake of easy visualisation\n",
    "pca = PCA(n_components= 2)\n",
    "# Fit and transform x to visualise inside a 2D feature space\n",
    "X_vis = pca.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE + Tomek links\n",
    "sm = SMOTETomek()\n",
    "X_resampled, y_resampled = sm.fit_sample(X, y)\n",
    "X_res_vis = pca.transform(X_resampled)\n",
    "\n",
    "# Two subplots, unpack the axes array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "c0 = ax1.scatter(X_vis[y == 0, 0], X_vis[y == 0, 1], label=\"Class #0\",\n",
    "                 alpha=0.5)\n",
    "c1 = ax1.scatter(X_vis[y == 1, 0], X_vis[y == 1, 1], label=\"Class #1\",\n",
    "                 alpha=0.5)\n",
    "ax1.set_title('Original set')\n",
    "\n",
    "ax2.scatter(X_res_vis[y_resampled == 0, 0], X_res_vis[y_resampled == 0, 1],\n",
    "            label=\"Class #0\", alpha=0.5)\n",
    "ax2.scatter(X_res_vis[y_resampled == 1, 0], X_res_vis[y_resampled == 1, 1],\n",
    "            label=\"Class #1\", alpha=0.5)\n",
    "ax2.set_title('SMOTE + Tomek')\n",
    "\n",
    "# make nice plotting\n",
    "for ax in (ax1, ax2):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    #ax.set_xlim([-6, 8])\n",
    "    #ax.set_ylim([-6, 6])\n",
    "\n",
    "plt.figlegend((c0, c1), ('Class #0', 'Class #1'), loc='lower center',\n",
    "              ncol=2, labelspacing=0.)\n",
    "plt.tight_layout(pad=3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the level of imbalanced data set after Smote Tomek application:\n",
    "# The results show the Smote Tomek function has actually increased the minority and achive a balanced data set\n",
    "\n",
    "count_d = 0\n",
    "count_nd = 0\n",
    "\n",
    "for i in y_resampled:\n",
    "    if i == 1:\n",
    "        count_d += 1\n",
    "    else:\n",
    "        count_nd += 1\n",
    "    \n",
    "total= count_d + count_nd\n",
    "print(\"Total cases under study\", total)\n",
    "print(\"Total default cases\", count_d,\"or \", round(count_d/total,2)*100,\"%\" )\n",
    "print(\"Total non-default cases\", count_nd,\"or \", round(count_nd/total,2)*100,\"%\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe for all the columns after the Credit_dummies data set was treated by Smote Tomek \n",
    "\n",
    "X= pd.DataFrame(X_resampled)\n",
    "y= pd.DataFrame(y_resampled)\n",
    "Credit_ST= pd.concat([X, y], axis= 1)\n",
    "Credit_ST.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call out the headers of Credit_dummies to compare aginst the headers of X_resampled\n",
    "\n",
    "Credit_dummies.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put headers of Credit_dummies.columns onto X_resampled and y_resampled\n",
    "\n",
    "initialcol = Credit_dummies.columns\n",
    "Credit_ST.columns = initialcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Credit_ST.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix X from the columns treated by Smote Tomek\n",
    "feature_cols= Credit_ST.columns.drop(['default_nextMTH'])\n",
    "X= Credit_ST[feature_cols]\n",
    "\n",
    "# Create response vector (y)\n",
    "\n",
    "y= Credit_ST.default_nextMTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_pred = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Credit_dummies data set has been treated by Smote Tomek. Use Logistic Regression to check their accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "model = logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Testing score: '  + str(logreg.score(X_test, y_test)))\n",
    "print('Training score: '  + str(logreg.score(X_train, y_train)))\n",
    "list(zip(feature_cols, logreg.feature_importances_)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance check does not show any particular feature having significant importance, in other words, \n",
    "the features are having similar weights on the target column.\n",
    "The train and test scores at about 52% only shows slightly better accuracy than the 50%:50% data set after treated \n",
    "by Smote Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "cm=metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"Actual: No\", \"Actual: Yes\"]\n",
    "list2 = [\"Predicted: No\", \"Predicted: Yes\"]\n",
    "pd.DataFrame(cm, list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('default_nextMTH Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using another sklearn linear model to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model, model_selection, metrics\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.25, random_state=46)\n",
    "logit_simple = linear_model.LogisticRegression(C=1e9).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1- y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our accuracy on the test set?\n",
    "print(np.mean(y_test == logit_simple.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions.\n",
    "logit_pred_proba = logit_simple.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_true=y_test, y_pred=logit_pred_proba > .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sklearn linear model is built on logistic regression and hence, is showing the same accuracy at about 52%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Apply RandomForest to get model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train model\n",
    "Random = RandomForestClassifier(n_estimators=10, max_depth= 12)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "model= Random.fit(X_train,y_train)\n",
    " \n",
    "# Predict on training set\n",
    "y_pred = Random.predict(X_test)\n",
    " \n",
    " \n",
    "print('Testing score: '  + str(Random.score(X_test, y_test)))\n",
    "print('Training score: '  + str(Random.score(X_train, y_train)))\n",
    "list(zip(feature_cols, Random.feature_importances_))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance check does not show any particular feature having significant importance, in other words, the features \n",
    "are having similar weights on the target column. \n",
    "The most importance featur is PAY_0 which makes sense as this is the latest bill payment. The least important feature\n",
    "is EDUCATION category 4, because of data availability issue.\n",
    "Random forest can work better with categorical features, than logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the test data set accuracy using confusion matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "cm=metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"Actual: No\", \"Actual: Yes\"]\n",
    "list2 = [\"Predicted: No\", \"Predicted: Yes\"]\n",
    "pd.DataFrame(cm, list1,list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision and recall accuracies are good after the data set are treated by Smote Tomek and get to balanced\n",
    "The Predictions on default (Positive) and no default (Negative) with correct outcomes are rather evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('default_nextMTH Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision and the recall are both very because the data set is now balanced.\n",
    "In other words, the model is good in predicting both default and none default cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(gamma='auto')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "model= svc.fit(X_train, y_train) \n",
    "\n",
    "y_pred= svc.predict(X_test)\n",
    "\n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "print('Testing score: '  + str(svc.score(X_test, y_test)))\n",
    "print('Training score: '  + str(svc.score(X_train, y_train)))\n",
    "list(zip(feature_cols, svc.feature_importances_))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "iris = load_iris()\n",
    "cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "                             \n",
    "\n",
    "#array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
    "        #0.93...,  0.93...,  1.     ,  0.93...,  1.      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
